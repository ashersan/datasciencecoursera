{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBk0ZDWY-ff8"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"http://introtodeeplearning.com\">\n",
        "        <img src=\"https://i.ibb.co/Jr88sn2/mit.png\" style=\"padding-bottom:5px;\" />\n",
        "      Visit MIT Deep Learning</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/MITDeepLearning/introtodeeplearning/blob/master/lab1/PT_Part1_Intro.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/MITDeepLearning/introtodeeplearning/blob/master/lab1/PT_Part1_Intro.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
        "</table>\n",
        "\n",
        "# Copyright Information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eI6DUic-6jo"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 MIT Introduction to Deep Learning. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the MIT License. You may not use this file except in compliance\n",
        "# with the License. Use and/or modification of this code outside of MIT Introduction\n",
        "# to Deep Learning must reference:\n",
        "#\n",
        "# Â© MIT Introduction to Deep Learning\n",
        "# http://introtodeeplearning.com\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57knM8jrYZ2t"
      },
      "source": [
        "# Lab 1: Intro to PyTorch and Music Generation with RNNs\n",
        "\n",
        "In this lab, you'll get exposure to using PyTorch and learn how it can be used for deep learning. Go through the code and run each cell. Along the way, you'll encounter several ***TODO*** blocks -- follow the instructions to fill them out before running those cells and continuing.\n",
        "\n",
        "\n",
        "# Part 1: Intro to PyTorch\n",
        "\n",
        "## 0.1 Install PyTorch\n",
        "\n",
        "[PyTorch](https://pytorch.org/) is a popular deep learning library known for its flexibility and ease of use. Here we'll learn how computations are represented and how to define a simple neural network in PyTorch. For all the labs in Introduction to Deep Learning 2025, there will be a PyTorch version available.\n",
        "\n",
        "Let's install PyTorch and a couple of dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LkaimNJfYZ2w"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade datasets gcsfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7op8B_knlqv4",
        "outputId": "c8dc831d-2f04-4a2e-ff3c-f085f75a52ef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.11/dist-packages (2025.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (4.4.2)\n",
            "INFO: pip is looking at multiple versions of gcsfs to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting gcsfs\n",
            "  Downloading gcsfs-2025.3.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading gcsfs-2025.3.1-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading gcsfs-2025.3.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading gcsfs-2025.2.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "INFO: pip is still looking at multiple versions of gcsfs to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading gcsfs-2024.12.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (from gcsfs) (1.2.1)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.19.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib->gcsfs) (2.0.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.24.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (1.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.69.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.26.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gcsfs-2024.12.0-py2.py3-none-any.whl (35 kB)\n",
            "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, gcsfs\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: gcsfs\n",
            "    Found existing installation: gcsfs 2025.3.2\n",
            "    Uninstalling gcsfs-2025.3.2:\n",
            "      Successfully uninstalled gcsfs-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 gcsfs-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and import the MIT Introduction to Deep Learning package\n",
        "!pip install --upgrade mitdeeplearning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ8BtfBskVbg",
        "outputId": "6a85f9d5-6674-44a9-e9ce-04f60318c77a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mitdeeplearning\n",
            "  Downloading mitdeeplearning-0.7.5.tar.gz (2.8 MB)\n",
            "\u001b[?25l     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m0.0/2.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ\u001b[0m\u001b[91mâ¸\u001b[0m\u001b[90mââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m0.1/2.8 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91mâââââââââââââââââ\u001b[0m\u001b[91mâ¸\u001b[0m\u001b[90mââââââââââââââââââââââ\u001b[0m \u001b[32m1.2/2.8 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mitdeeplearning) (2.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from mitdeeplearning) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mitdeeplearning) (4.67.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (from mitdeeplearning) (0.25.2)\n",
            "Collecting opik (from mitdeeplearning)\n",
            "  Downloading opik-1.6.13-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from mitdeeplearning) (1.70.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from mitdeeplearning) (4.50.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from mitdeeplearning) (3.5.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (from mitdeeplearning) (0.14.0)\n",
            "Collecting lion-pytorch (from mitdeeplearning)\n",
            "  Downloading lion_pytorch-0.2.3-py3-none-any.whl.metadata (616 bytes)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets->mitdeeplearning) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->mitdeeplearning) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->mitdeeplearning) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->mitdeeplearning) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets->mitdeeplearning) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->mitdeeplearning) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->mitdeeplearning) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->mitdeeplearning) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->mitdeeplearning) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets->mitdeeplearning) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->mitdeeplearning) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->mitdeeplearning) (6.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym->mitdeeplearning) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym->mitdeeplearning) (0.0.8)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.11/dist-packages (from lion-pytorch->mitdeeplearning) (2.6.0+cu124)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai->mitdeeplearning) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->mitdeeplearning) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai->mitdeeplearning) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->mitdeeplearning) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai->mitdeeplearning) (2.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai->mitdeeplearning) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai->mitdeeplearning) (4.13.0)\n",
            "Collecting boto3-stubs>=1.34.110 (from boto3-stubs[bedrock-runtime]>=1.34.110->opik->mitdeeplearning)\n",
            "  Downloading boto3_stubs-1.37.28-py3-none-any.whl.metadata (149 kB)\n",
            "\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m149.3/149.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from opik->mitdeeplearning) (8.1.8)\n",
            "Collecting levenshtein<1.0.0 (from opik->mitdeeplearning)\n",
            "  Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting litellm (from opik->mitdeeplearning)\n",
            "  Downloading litellm-1.65.4.post1.tar.gz (6.7 MB)\n",
            "\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pydantic-settings<3.0.0,>=2.0.0 (from opik->mitdeeplearning)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from opik->mitdeeplearning) (8.3.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from opik->mitdeeplearning) (13.9.4)\n",
            "Requirement already satisfied: sentry_sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from opik->mitdeeplearning) (2.25.1)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.11/dist-packages (from opik->mitdeeplearning) (9.1.2)\n",
            "Collecting uuid6 (from opik->mitdeeplearning)\n",
            "  Downloading uuid6-2024.7.10-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from opik->mitdeeplearning) (3.1.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft->mitdeeplearning) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft->mitdeeplearning) (1.5.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft->mitdeeplearning) (0.5.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->mitdeeplearning) (0.21.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai->mitdeeplearning) (3.10)\n",
            "Collecting botocore-stubs (from boto3-stubs>=1.34.110->boto3-stubs[bedrock-runtime]>=1.34.110->opik->mitdeeplearning)\n",
            "  Downloading botocore_stubs-1.37.28-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting types-s3transfer (from boto3-stubs>=1.34.110->boto3-stubs[bedrock-runtime]>=1.34.110->opik->mitdeeplearning)\n",
            "  Downloading types_s3transfer-0.11.4-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting mypy-boto3-bedrock-runtime<1.38.0,>=1.37.0 (from boto3-stubs[bedrock-runtime]>=1.34.110->opik->mitdeeplearning)\n",
            "  Downloading mypy_boto3_bedrock_runtime-1.37.24-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->mitdeeplearning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->mitdeeplearning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->mitdeeplearning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->mitdeeplearning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->mitdeeplearning) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->mitdeeplearning) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->mitdeeplearning) (1.18.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->mitdeeplearning) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->mitdeeplearning) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->mitdeeplearning) (0.14.0)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from levenshtein<1.0.0->opik->mitdeeplearning)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai->mitdeeplearning) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai->mitdeeplearning) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai->mitdeeplearning) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.0.0->opik->mitdeeplearning)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->mitdeeplearning) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->mitdeeplearning) (2.3.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->lion-pytorch->mitdeeplearning) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.6->lion-pytorch->mitdeeplearning)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.6->lion-pytorch->mitdeeplearning)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.6->lion-pytorch->mitdeeplearning)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.6->lion-pytorch->mitdeeplearning)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.6->lion-pytorch->mitdeeplearning)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.6->lion-pytorch->mitdeeplearning)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.6->lion-pytorch->mitdeeplearning)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.6->lion-pytorch->mitdeeplearning)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.6->lion-pytorch->mitdeeplearning)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->lion-pytorch->mitdeeplearning) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->lion-pytorch->mitdeeplearning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->lion-pytorch->mitdeeplearning) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.6->lion-pytorch->mitdeeplearning)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->lion-pytorch->mitdeeplearning) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->lion-pytorch->mitdeeplearning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6->lion-pytorch->mitdeeplearning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->opik->mitdeeplearning) (3.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm->opik->mitdeeplearning) (8.6.1)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm->opik->mitdeeplearning) (4.23.0)\n",
            "Collecting tiktoken>=0.7.0 (from litellm->opik->mitdeeplearning)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->mitdeeplearning) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->mitdeeplearning) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->mitdeeplearning) (2025.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->opik->mitdeeplearning) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->opik->mitdeeplearning) (1.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->opik->mitdeeplearning) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->opik->mitdeeplearning) (2.18.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm->opik->mitdeeplearning) (3.21.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm->opik->mitdeeplearning) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm->opik->mitdeeplearning) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm->opik->mitdeeplearning) (0.24.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->opik->mitdeeplearning) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->mitdeeplearning) (1.17.0)\n",
            "Collecting types-awscrt (from botocore-stubs->boto3-stubs>=1.34.110->boto3-stubs[bedrock-runtime]>=1.34.110->opik->mitdeeplearning)\n",
            "  Downloading types_awscrt-0.25.7-py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading lion_pytorch-0.2.3-py3-none-any.whl (6.6 kB)\n",
            "Downloading opik-1.6.13-py3-none-any.whl (438 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m438.6/438.6 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3_stubs-1.37.28-py3-none-any.whl (68 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uuid6-2024.7.10-py3-none-any.whl (6.4 kB)\n",
            "Downloading mypy_boto3_bedrock_runtime-1.37.24-py3-none-any.whl (31 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore_stubs-1.37.28-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_s3transfer-0.11.4-py3-none-any.whl (19 kB)\n",
            "Downloading types_awscrt-0.25.7-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: mitdeeplearning, litellm\n",
            "  Building wheel for mitdeeplearning (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mitdeeplearning: filename=mitdeeplearning-0.7.5-py3-none-any.whl size=2853019 sha256=3c8bb165b98273e6dc3c8c5e1360a5ba6e9d868285731fcc5a9ffaec80cdc5f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/39/36/86e4ccc61c368cfc06712313ea7c15134e76ecefe412677156\n",
            "  Building wheel for litellm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for litellm: filename=litellm-1.65.4.post1-py3-none-any.whl size=7076116 sha256=e7119b661a42760df647ffc8364ff1d8d57236bfa59b59526e88f6a472e90b8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/5a/23/45b9efe33d441fd727cd3d07a972800c73c6dd190f78a0bc28\n",
            "Successfully built mitdeeplearning litellm\n",
            "Installing collected packages: uuid6, types-s3transfer, types-awscrt, rapidfuzz, python-dotenv, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-boto3-bedrock-runtime, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, levenshtein, botocore-stubs, pydantic-settings, nvidia-cusolver-cu12, boto3-stubs, litellm, opik, lion-pytorch, mitdeeplearning\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed boto3-stubs-1.37.28 botocore-stubs-1.37.28 levenshtein-0.27.1 lion-pytorch-0.2.3 litellm-1.65.4.post1 mitdeeplearning-0.7.5 mypy-boto3-bedrock-runtime-1.37.24 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 opik-1.6.13 pydantic-settings-2.8.1 python-dotenv-1.1.0 rapidfuzz-3.13.0 tiktoken-0.9.0 types-awscrt-0.25.7 types-s3transfer-0.11.4 uuid6-2024.7.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mitdeeplearning as mdl"
      ],
      "metadata": {
        "id": "cSJPkDx1kiOL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QNMcdP4m3Vs"
      },
      "source": [
        "## 1.1 What is PyTorch?\n",
        "\n",
        "PyTorch is a machine learning library, like TensorFlow. At its core, PyTorch provides an interface for creating and manipulating [tensors](https://pytorch.org/docs/stable/tensors.html), which are data structures that you can think of as multi-dimensional arrays. Tensors are represented as n-dimensional arrays of base datatypes such as a string or integer -- they provide a way to generalize vectors and matrices to higher dimensions. PyTorch provides the ability to perform computation on these tensors, define neural networks, and train them efficiently.\n",
        "\n",
        "The [```shape```](https://pytorch.org/docs/stable/generated/torch.Tensor.shape.html#torch.Tensor.shape) of a PyTorch tensor defines its number of dimensions and the size of each dimension. The `ndim` or [```dim```](https://pytorch.org/docs/stable/generated/torch.Tensor.dim.html#torch.Tensor.dim) of a PyTorch tensor provides the number of dimensions (n-dimensions) -- this is equivalent to the tensor's rank (as is used in TensorFlow), and you can also think of this as the tensor's order or degree.\n",
        "\n",
        "Letâs start by creating some tensors and inspecting their properties:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tFxztZQInlAB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f899e1f-54f4-4b41-8bad-a79fd0b613d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`integer` is a 0-d Tensor: 1234\n",
            "`decimal` is a 0-d Tensor: 3.141590118408203\n"
          ]
        }
      ],
      "source": [
        "integer = torch.tensor(1234)\n",
        "#decimal = torch.tensor(3.14159265359)\n",
        "decimal = torch.tensor(3.14159)\n",
        "\n",
        "\n",
        "print(f\"`integer` is a {integer.ndim}-d Tensor: {integer}\")\n",
        "print(f\"`decimal` is a {decimal.ndim}-d Tensor: {decimal}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dljcPUcoJZ6"
      },
      "source": [
        "Vectors and lists can be used to create 1-d tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oaHXABe8oPcO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97b85b11-5610-4270-a51a-7fce300b081b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`fibonacci` is a 1-d Tensor with shape: torch.Size([6])\n",
            "`count_to_100` is a 1-d Tensor with shape: torch.Size([100])\n"
          ]
        }
      ],
      "source": [
        "fibonacci = torch.tensor([1, 1, 2, 3, 5, 8])\n",
        "count_to_100 = torch.tensor(range(100))\n",
        "\n",
        "print(f\"`fibonacci` is a {fibonacci.ndim}-d Tensor with shape: {fibonacci.shape}\")\n",
        "print(f\"`count_to_100` is a {count_to_100.ndim}-d Tensor with shape: {count_to_100.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvffwkvtodLP"
      },
      "source": [
        "Next, letâs create 2-d (i.e., matrices) and higher-rank tensors. In image processing and computer vision, we will use 4-d Tensors with dimensions corresponding to batch size, number of color channels, image height, and image width."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tFeBBe1IouS3"
      },
      "outputs": [],
      "source": [
        "### Defining higher-order Tensors ###\n",
        "\n",
        "'''TODO: Define a 2-d Tensor'''\n",
        "matrix = torch.tensor([[1, 2, 3 , 4], [5, 6, 7, 8 ]])# TODO\n",
        "\n",
        "assert isinstance(matrix, torch.Tensor), \"matrix must be a torch Tensor object\"\n",
        "assert matrix.ndim == 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''TODO: Define a 4-d Tensor.'''\n",
        "# Use torch.zeros to initialize a 4-d Tensor of zeros with size 10 x 3 x 256 x 256.\n",
        "#   You can think of this as 10 images where each image is RGB 256 x 256.\n",
        "images = torch.zeros(10, 3, 256, 256)\n",
        "\n",
        "assert isinstance(images, torch.Tensor), \"images must be a torch Tensor object\"\n",
        "assert images.ndim == 4, \"images must have 4 dimensions\"\n",
        "assert images.shape == (10, 3, 256, 256), \"images is incorrect shape\"\n",
        "print(f\"images is a {images.ndim}-d Tensor with shape: {images.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROg-qTHposx4",
        "outputId": "1714b854-f1c6-47cb-c67b-23556b1394b0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images is a 4-d Tensor with shape: torch.Size([10, 3, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkaCDOGapMyl"
      },
      "source": [
        "As you have seen, the `shape` of a tensor provides the number of elements in each tensor dimension. The `shape` is quite useful, and we'll use it often. You can also use slicing to access subtensors within a higher-rank tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FhaufyObuLEG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcd0f2b1-a1a3-4d43-f147-dda0ba210ac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`row_vector`: tensor([5, 6, 7, 8])\n",
            "`column_vector`: tensor([2, 6])\n",
            "`scalar`: 2\n",
            "tensor([[1, 2, 3, 4],\n",
            "        [5, 6, 7, 8]])\n"
          ]
        }
      ],
      "source": [
        "row_vector = matrix[1]\n",
        "column_vector = matrix[:, 1]\n",
        "scalar = matrix[0, 1]\n",
        "\n",
        "print(f\"`row_vector`: {row_vector}\")\n",
        "print(f\"`column_vector`: {column_vector}\")\n",
        "print(f\"`scalar`: {scalar}\")\n",
        "print(matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD3VO-LZYZ2z"
      },
      "source": [
        "## 1.2 Computations on Tensors\n",
        "\n",
        "A convenient way to think about and visualize computations in a machine learning framework like PyTorch is in terms of graphs. We can define this graph in terms of tensors, which hold data, and the mathematical operations that act on these tensors in some order. Let's look at a simple example, and define this computation using PyTorch:\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/MITDeepLearning/introtodeeplearning/2025/lab1/img/add-graph.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "X_YJrZsxYZ2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f9e7aaf-37a5-461e-9d0f-433a67f5b8b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c1: 76\n",
            "c2: 76\n"
          ]
        }
      ],
      "source": [
        "# Create the nodes in the graph and initialize values\n",
        "a = torch.tensor(15)\n",
        "b = torch.tensor(61)\n",
        "\n",
        "# Add them!\n",
        "c1 = torch.add(a, b)\n",
        "c2 = a + b  # PyTorch overrides the \"+\" operation so that it is able to act on Tensors\n",
        "print(f\"c1: {c1}\")\n",
        "print(f\"c2: {c2}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mbfv_QOiYZ23"
      },
      "source": [
        "Notice how we've created a computation graph consisting of PyTorch operations, and how the output is a tensor with value 76 -- we've just created a computation graph consisting of operations, and it's executed them and given us back the result.\n",
        "\n",
        "Now let's consider a slightly more complicated example:\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/MITDeepLearning/introtodeeplearning/2025/lab1/img/computation-graph.png)\n",
        "\n",
        "Here, we take two inputs, `a, b`, and compute an output `e`. Each node in the graph represents an operation that takes some input, does some computation, and passes its output to another node.\n",
        "\n",
        "Let's define a simple function in PyTorch to construct this computation function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PJnfzpWyYZ23",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "### Defining Tensor computations ###\n",
        "\n",
        "# Construct a simple computation function\n",
        "def func(a, b):\n",
        "    '''TODO: Define the operation for c, d, e.'''\n",
        "    c = torch.add(a, b)# TODO\n",
        "    print(f\"c:{c}\")\n",
        "    d = torch.subtract(b, 1)# TODO\n",
        "    print(f\"d:{d}\")\n",
        "    e = torch.multiply(c,d)# TODO\n",
        "    print(f\"e:{e}\")\n",
        "    return e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwrRfDMS2-oy"
      },
      "source": [
        "Now, we can call this function to execute the computation graph given some inputs `a,b`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pnwsf8w2uF7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "638d2e40-c5dd-40a8-ddde-de6f066a034e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c:4.0\n",
            "d:1.5\n",
            "e:6.0\n",
            "e_out: 6.0\n"
          ]
        }
      ],
      "source": [
        "# Consider example values for a,b\n",
        "a, b = 1.5, 2.5\n",
        "# Execute the computation\n",
        "e_out = func(a, b)\n",
        "print(f\"e_out: {e_out}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HqgUIUhYZ29"
      },
      "source": [
        "Notice how our output is a tensor with value defined by the output of the computation, and that the output has no shape as it is a single scalar value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h4o9Bb0YZ29"
      },
      "source": [
        "## 1.3 Neural networks in PyTorch\n",
        "We can also define neural networks in PyTorch. PyTorch uses [``torch.nn.Module``](https://pytorch.org/docs/stable/generated/torch.nn.Module.html), which serves as a base class for all neural network modules in PyTorch and thus provides a framework for building and training neural networks.\n",
        "\n",
        "Let's consider the example of a simple perceptron defined by just one dense (aka fully-connected or linear) layer: $ y = \\sigma(Wx + b) $, where $W$ represents a matrix of weights, $b$ is a bias, $x$ is the input, $\\sigma$ is the sigmoid activation function, and $y$ is the output.\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/MITDeepLearning/introtodeeplearning/2025/lab1/img/computation-graph-2.png)\n",
        "\n",
        "We will use `torch.nn.Module` to define layers -- the building blocks of neural networks. Layers implement common neural networks operations. In PyTorch, when we implement a layer, we subclass `nn.Module` and define the parameters of the layer as attributes of our new class. We also define and override a function [``forward``](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward), which will define the forward pass computation that is performed at every step. All classes subclassing `nn.Module` should override the `forward` function.\n",
        "\n",
        "Let's write a dense layer class to implement a perceptron defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HutbJk-1kHPh"
      },
      "outputs": [],
      "source": [
        "### Defining a dense layer ###\n",
        "\n",
        "# num_inputs: number of input nodes\n",
        "# num_outputs: number of output nodes\n",
        "# x: input to the layer\n",
        "\n",
        "class OurDenseLayer(torch.nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs):\n",
        "        super(OurDenseLayer, self).__init__()\n",
        "        # Define and initialize parameters: a weight matrix W and bias b\n",
        "        # Note that the parameter initialize is random!\n",
        "        self.W = torch.nn.Parameter(torch.randn(num_inputs, num_outputs))\n",
        "        self.bias = torch.nn.Parameter(torch.randn(num_outputs))\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''TODO: define the operation for z (hint: use torch.matmul).'''\n",
        "        z = torch.matmul(x, self.W) + self.bias # TODO\n",
        "\n",
        "        '''TODO: define the operation for out (hint: use torch.sigmoid).'''\n",
        "        y = torch.sigmoid(z) # TODO\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqeEbn959hV_"
      },
      "source": [
        "Now, let's test the output of our layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2yxjCPa69hV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f28042b4-e1a7-47cd-daff-dcec2a039ae6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2.]])\n",
            "input shape: torch.Size([1, 2])\n",
            "output shape: torch.Size([1, 3])\n",
            "output result: tensor([[0.9681, 0.8311, 0.4777]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Define a layer and test the output!\n",
        "num_inputs = 2\n",
        "num_outputs = 3\n",
        "layer = OurDenseLayer(num_inputs, num_outputs)\n",
        "x_input = torch.tensor([[1, 2.]])\n",
        "print(x_input)\n",
        "y = layer(x_input)\n",
        "\n",
        "print(f\"input shape: {x_input.shape}\")\n",
        "print(f\"output shape: {y.shape}\")\n",
        "print(f\"output result: {y}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt1FgM7qYZ3D"
      },
      "source": [
        "Conveniently, PyTorch has defined a number of ```nn.Modules``` (or Layers) that are commonly used in neural networks, for example a [```nn.Linear```](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) or [`nn.Sigmoid`](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) module.\n",
        "\n",
        "Now, instead of using a single ```Module``` to define our simple neural network, we'll use the  [`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) module from PyTorch and a single [`nn.Linear` ](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer to define our network. With the `Sequential` API, you can readily create neural networks by stacking together layers like building blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7WXTpmoL6TDz"
      },
      "outputs": [],
      "source": [
        "### Defining a neural network using the PyTorch Sequential API ###\n",
        "\n",
        "# define the number of inputs and outputs\n",
        "n_input_nodes = 2\n",
        "n_output_nodes = 3\n",
        "\n",
        "# Define the model\n",
        "'''TODO: Use the Sequential API to define a neural network with a\n",
        "    single linear (dense!) layer, followed by non-linearity to compute z'''\n",
        "# model = nn.Sequential( ''' TODO ''' )\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(n_input_nodes, n_output_nodes),  # Linear (dense) layer\n",
        "    nn.Sigmoid()                               # Non-linear activation\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDGcwYfUyR-U"
      },
      "source": [
        "We've defined our model using the Sequential API. Now, we can test it out using an example input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zKhp6XqCFFa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5c07f8b-e507-44d5-c9d9-949bcf426c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input shape: torch.Size([1, 2])\n",
            "output shape: torch.Size([1, 3])\n",
            "output result: tensor([[0.9681, 0.8311, 0.4777]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Test the model with example input\n",
        "x_input = torch.tensor([[1, 2.]])\n",
        "model_output = model(x_input)\n",
        "print(f\"input shape: {x_input.shape}\")\n",
        "print(f\"output shape: {y.shape}\")\n",
        "print(f\"output result: {y}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "596NvsOOtr9F"
      },
      "source": [
        "With PyTorch, we can create more flexible models by subclassing [`nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). The `nn.Module` class allows us to group layers together flexibly to define new architectures.\n",
        "\n",
        "As we saw earlier with `OurDenseLayer`, we can subclass `nn.Module` to create a class for our model, and then define the forward pass through the network using the `forward` function. Subclassing affords the flexibility to define custom layers, custom training loops, custom activation functions, and custom models. Let's define the same neural network model as above (i.e., Linear layer with an activation function after it), now using subclassing and using PyTorch's built in linear layer from `nn.Linear`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "K4aCflPVyViD"
      },
      "outputs": [],
      "source": [
        "### Defining a model using subclassing ###\n",
        "\n",
        "class LinearWithSigmoidActivation(nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs):\n",
        "        super(LinearWithSigmoidActivation, self).__init__()\n",
        "        '''TODO: define a model with a single Linear layer and sigmoid activation.'''\n",
        "        '''TODO: linear layer'''\n",
        "        self.linear = nn.Linear(num_inputs, num_outputs)\n",
        "        '''TODO: sigmoid activation'''\n",
        "        self.activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        linear_output = self.linear(inputs)\n",
        "        output = self.activation(linear_output)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goKCQ9dEGzRn"
      },
      "source": [
        "Let's test out our new model, using an example input, setting `n_input_nodes=2` and `n_output_nodes=3` as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "V-eNhSyRG6hl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c496d6a8-35e8-4f95-93b6-80babc7b7b96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input shape: torch.Size([1, 2])\n",
            "output shape: torch.Size([1, 3])\n",
            "output result: tensor([[0.4301, 0.3449, 0.0883]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ],
      "source": [
        "n_input_nodes = 2\n",
        "n_output_nodes = 3\n",
        "model = LinearWithSigmoidActivation(n_input_nodes, n_output_nodes)\n",
        "x_input = torch.tensor([[1, 2.]])\n",
        "y = model(x_input)\n",
        "print(f\"input shape: {x_input.shape}\")\n",
        "print(f\"output shape: {y.shape}\")\n",
        "print(f\"output result: {y}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTIFMJLAzsyE"
      },
      "source": [
        "Importantly, `nn.Module` affords us a lot of flexibility to define custom models. For example, we can use boolean arguments in the `forward` function to specify different network behaviors, for example different behaviors during training and inference. Let's suppose under some instances we want our network to simply output the input, without any perturbation. We define a boolean argument `isidentity` to control this behavior:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "P7jzGX5D1xT5"
      },
      "outputs": [],
      "source": [
        "''\n",
        "### Custom behavior with subclassing nn.Module ###\n",
        "\n",
        "class LinearButSometimesIdentity(nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs):\n",
        "        super(LinearButSometimesIdentity, self).__init__()\n",
        "        self.linear = nn.Linear(num_inputs, num_outputs)\n",
        "\n",
        "    '''TODO: Implement the behavior where the network outputs the input, unchanged,\n",
        "        under control of the isidentity argument.'''\n",
        "#    def forward(self, inputs, isidentity=False):\n",
        "#      ''' TODO '\n",
        "    def forward(self, inputs, isidentity=False):\n",
        "        if isidentity:\n",
        "            # Return input unchanged (identity function)\n",
        "            # Note: We ensure the output shape matches what the linear layer would produce\n",
        "            if inputs.shape[-1] == self.linear.out_features:\n",
        "                return inputs\n",
        "            else:\n",
        "                # If dimensions don't match, return zeros with correct output shape\n",
        "                return torch.zeros(*inputs.shape[:-1], self.linear.out_features,\n",
        "                                device=inputs.device, dtype=inputs.dtype)\n",
        "        else:\n",
        "            # Normal linear transformation\n",
        "            return self.linear(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku4rcCGx5T3y"
      },
      "source": [
        "Let's test this behavior:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NzC0mgbk5dp2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3441e2a7-3934-4224-881b-03bd49c5c873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: tensor([[1., 2.]])\n",
            "Network linear output: tensor([[-1.3667e+00,  7.5817e-01,  2.9764e-04]], grad_fn=<AddmmBackward0>); network identity output: tensor([[0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "# Test the IdentityModel\n",
        "model = LinearButSometimesIdentity(num_inputs=2, num_outputs=3)\n",
        "x_input = torch.tensor([[1, 2.]])\n",
        "\n",
        "'''TODO: pass the input into the model and call with and without the input identity option.'''\n",
        "out_with_linear = model(x_input )# TODO\n",
        "\n",
        "out_with_identity = model(x_input, isidentity=True )# TODO\n",
        "\n",
        "print(f\"input: {x_input}\")\n",
        "print(\"Network linear output: {}; network identity output: {}\".format(out_with_linear, out_with_identity))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V1dEqdk6VI5"
      },
      "source": [
        "Now that we have learned how to define layers and models in PyTorch using both the Sequential API and subclassing `nn.Module`, we're ready to turn our attention to how to actually implement network training with backpropagation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQwDhKn8kbO2"
      },
      "source": [
        "## 1.4 Automatic Differentiation in PyTorch\n",
        "\n",
        "In PyTorch, [`torch.autograd`](https://pytorch.org/docs/stable/autograd.html) is used for [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation), which is critical for training deep learning models with [backpropagation](https://en.wikipedia.org/wiki/Backpropagation).\n",
        "\n",
        "We will use the PyTorch [`.backward()`](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html) method to trace operations for computing gradients. On a tensor, the [`requires_grad`](https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad_.html) attribute controls whether autograd should record operations on that tensor. When a forward pass is made through the network, PyTorch builds a computational graph dynamically; then, to compute the gradient, the `backward()` method is called to perform backpropagation.\n",
        "\n",
        "Let's compute the gradient of $ y = x^2 $:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tdkqk8pw5yJM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ebdcc25-6eb9-4398-e9a0-4566c6236124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dy_dx of y=x^2 at x=3.0 is:  tensor(6.)\n"
          ]
        }
      ],
      "source": [
        "### Gradient computation ###\n",
        "\n",
        "# y = x^2\n",
        "# Example: x = 3.0\n",
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "y = x ** 2\n",
        "y.backward()  # Compute the gradient\n",
        "\n",
        "dy_dx = x.grad\n",
        "print(\"dy_dx of y=x^2 at x=3.0 is: \", dy_dx)\n",
        "assert dy_dx == 6.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhU5metS5xF3"
      },
      "source": [
        "In training neural networks, we use differentiation and stochastic gradient descent (SGD) to optimize a loss function. Now that we have a sense of how PyTorch's autograd can be used to compute and access derivatives, we will look at an example where we use automatic differentiation and SGD to find the minimum of $ L=(x-x_f)^2 $. Here $x_f$ is a variable for a desired value we are trying to optimize for; $L$ represents a loss that we are trying to minimize. While we can clearly solve this problem analytically ($ x_{min}=x_f $), considering how we can compute this using PyTorch's autograd sets us up nicely for future labs where we use gradient descent to optimize entire neural network losses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "attributes": {
          "classes": [
            "py"
          ],
          "id": ""
        },
        "id": "7g1yWiSXqEf-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "0f79c280-69bd-4e90-eff3-89610bd69e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing x=-1.0552258491516113\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "### Function minimization with autograd and gradient descent ###\n",
        "\n",
        "# Initialize a random value for our intial x\n",
        "x = torch.randn(1)\n",
        "print(f\"Initializing x={x.item()}\")\n",
        "\n",
        "learning_rate = 1e-2  # Learning rate\n",
        "history = []\n",
        "x_f = 4  # Target value\n",
        "\n",
        "\n",
        "# We will run gradient descent for a number of iterations. At each iteration, we compute the loss,\n",
        "#   compute the derivative of the loss with respect to x, and perform the update.\n",
        "for i in range(500):\n",
        "    x = torch.tensor([x], requires_grad=True)\n",
        "\n",
        "    # TODO: Compute the loss as the square of the difference between x and x_f\n",
        "    loss = (x - x_f)**2 # TODO\n",
        "\n",
        "    # Backpropagate through the loss to compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Update x with gradient descent\n",
        "    x = x.item() - learning_rate * x.grad\n",
        "\n",
        "    history.append(x.item())\n",
        "\n",
        "# Plot the evolution of x as we optimize toward x_f!\n",
        "plt.plot(history)\n",
        "plt.plot([0, 500], [x_f, x_f])\n",
        "plt.legend(('Predicted', 'True'))\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('x value')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC7czCwk3ceH"
      },
      "source": [
        "Now, we have covered the fundamental concepts of PyTorch -- tensors, operations, neural networks, and automatic differentiation. Fire!!\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "WBk0ZDWY-ff8"
      ],
      "name": "PT_Part1_Intro.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}